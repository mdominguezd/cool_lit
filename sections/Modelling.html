<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>modelling – Lit literature review</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-bd89562929948232a0cfc2a2280e30cc.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Lit literature review</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../sections/first_chapter.html"> 
<span class="menu-text">Chapter 1</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../sections/GEDI.html"> 
<span class="menu-text">GEDI &amp; Lidar</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../sections/forest_fires.html"> 
<span class="menu-text">Forest fires</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../sections/other_satellites.html"> 
<span class="menu-text">Other satellites</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="../sections/Modelling.html" aria-current="page"> 
<span class="menu-text">Modelling</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../sections/knowledge_gaps.html"> 
<span class="menu-text">Knowledge gap</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#global-canopy-height-regression-and-uncertainty-estimation-from-gedi-lidar-waveforms-with-deep-ensembles" id="toc-global-canopy-height-regression-and-uncertainty-estimation-from-gedi-lidar-waveforms-with-deep-ensembles" class="nav-link active" data-scroll-target="#global-canopy-height-regression-and-uncertainty-estimation-from-gedi-lidar-waveforms-with-deep-ensembles">Global canopy height regression and uncertainty estimation from GEDI LIDAR waveforms with deep ensembles</a></li>
  <li><a href="#a-high-resolution-canopy-height-model-of-the-earth" id="toc-a-high-resolution-canopy-height-model-of-the-earth" class="nav-link" data-scroll-target="#a-high-resolution-canopy-height-model-of-the-earth">A high-resolution canopy height model of the Earth</a></li>
  <li><a href="#satclip-global-general-purpose-location-embeddings-with-satellite-imagery" id="toc-satclip-global-general-purpose-location-embeddings-with-satellite-imagery" class="nav-link" data-scroll-target="#satclip-global-general-purpose-location-embeddings-with-satellite-imagery">SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="global-canopy-height-regression-and-uncertainty-estimation-from-gedi-lidar-waveforms-with-deep-ensembles" class="level2">
<h2 class="anchored" data-anchor-id="global-canopy-height-regression-and-uncertainty-estimation-from-gedi-lidar-waveforms-with-deep-ensembles">Global canopy height regression and uncertainty estimation from GEDI LIDAR waveforms with deep ensembles</h2>
</section>
<section id="a-high-resolution-canopy-height-model-of-the-earth" class="level2">
<h2 class="anchored" data-anchor-id="a-high-resolution-canopy-height-model-of-the-earth">A high-resolution canopy height model of the Earth</h2>
</section>
<section id="satclip-global-general-purpose-location-embeddings-with-satellite-imagery" class="level2">
<h2 class="anchored" data-anchor-id="satclip-global-general-purpose-location-embeddings-with-satellite-imagery">SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery</h2>
<ol type="1">
<li>Introduction
<ul>
<li>Satellite imagery is a valuable input data for predictive models across a wide range of real-world applications.</li>
<li>Many geospatial models also directly leverage geographic location for improving predictions.</li>
<li>Integrating geographic information into a deep learning model is not straightforward.</li>
<li>Introducing spatial coordinates as features can amplify geographic distribution shift problems and lead to poor evaluation accuracy.</li>
<li>Many location-informed models are only applicable for interpolation problems where the evaluation area overlaps with the training area.</li>
<li>This work addresses the research problem by pretraining location encoders with globally and uniformly sampled satellite imagery and with general-purpose use in mind.</li>
</ul></li>
<li>Satellite Contrastive Location-Image Pretraining
<ul>
<li>SatCLIP learns an implicit representation of locations by matching CNN and ViT inferred visual patterns of openly available satellite imagery with their geographic coordinates.</li>
<li>The resulting location encoder efficiently summarizes the characteristics of any given location for convenient use in downstream tasks.</li>
<li>The paper motivates SatCLIP and then outlines its components and training paradigm.</li>
<li>SatCLIP has the potential to provide the best of both worlds: location embeddings capture both spatial effects and ground conditions while also being low-dimensional and runtime-efficient.</li>
</ul>
<ol type="1">
<li>Pretraining with the SatCLIP Objective
<ul>
<li>Describes the inputs to the geographic location encoder.</li>
<li>Defines two encoders: a location encoder and an image encoder.</li>
<li>Describes the CLIP objective for training both encoders.</li>
<li>This objective simultaneously optimizes the weights of the location encoder and image encoder to embed the feature vectors of the corresponding location and image nearby in a common d-dimensional feature space.</li>
</ul></li>
<li>Encoder Architectures
<ul>
<li>Geographic location encoders typically take the form of a small neural network (NN) applied to a nonparametric, functional positional encoding of the coordinates.</li>
<li>This work trains Siren(SH) location encoders proposed by Rußwurm et al., which use spherical harmonics basis (SH) functions as positional encoders and are particularly well-suited for coordinates on spherical surfaces.</li>
<li>They are combined with sinusoidal representation networks (Siren), which are broadly used for implicit neural representations.</li>
<li>The spatial smoothness of the representation is controlled by the number of Legendre polynomials L.</li>
<li>As an image encoder, they need a vision model that is expressive enough to learn visual patterns from satellite images.</li>
<li>In this work, they use ResNet18, ResNet50, and ViT16 vision encoders pretrained with momentum-contrast (MoCo) on Sentinel-2 satellite imagery.</li>
</ul></li>
<li>SatCLIP Implementation Details
<ul>
<li>SatCLIP is pretrained using the S2-100K dataset, which is assembled for this purpose.</li>
<li>90% of the data points, selected uniformly at random, are used for pretraining and the remaining 10% is reserved as a validation set to monitor overfitting.</li>
<li>Models are trained for 500 epochs on an A100 GPU.</li>
</ul></li>
</ol></li>
<li>Experimental Setup
<ul>
<li>The paper focuses on three research questions:</li>
<li>How generalizable are SatCLIP embeddings from Sentinel-2 data, both across a diverse range of geospatial modeling tasks (RQ 1) and across unseen geographic areas (RQ 2), compared to existing pretrained location encoders and location-only prediction?</li>
<li>Do SatCLIP embeddings capture ground conditions and incorporate similarities over space? (RQ 3).</li>
<li>Experiments are designed to test the performance of SatCLIP embeddings for downstream tasks, both for spatial interpolation, and for geographic domain generalization.</li>
<li>Geographic generalization is an important aspect of performance as distributional shifts across geographic areas are a common challenge in environmental problems.</li>
</ul>
<ol type="1">
<li>Pretraining Dataset: 100k uniformly sampled Sentinel-2 images
<ul>
<li>To construct their pretraining dataset, S2-100K, they sample 100,000 tiles of 256×256 pixel, multi-spectral (12-channel) Sentinel-2 satellite imagery and their associated centroid locations.</li>
<li>The S2-100K dataset is designed with the goals of multi-task applicability and geographic generalization performance in mind.</li>
<li>Unlike S2-100K, pretraining datasets used in comparison methods often significantly underrepresent certain—especially non-Western—geographic areas as they were not specifically designed to provide general-purpose embeddings.</li>
</ul></li>
<li>Downstream Tasks
<ul>
<li>To test the general applicability of SatCLIP location embeddings, experiments are run on a wide range of geospatial predictive modeling tasks.</li>
<li>The nine downstream datasets used in this work span socioeconomic and environmental applications.</li>
</ul></li>
<li>Comparison Methods
<ul>
<li>The trained SatCLIP location embeddings are compared to GPS2Vec, CSP, and GeoCLIP pretrained location embeddings.</li>
<li>To compare to an image-only embedding, globally precomputed MOSAIKS features are used.</li>
<li>To assess the performance improvement from the integration of contextual information over location-only prediction, they also compare to downstream learners trained on raw latitude/longitude coordinates.</li>
</ul></li>
<li>Downstream Model Training
<ul>
<li>For all downstream tasks, multi-layer perceptron (MLP) models are trained with location embeddings and raw latitude/longitude coordinates as input to predict a (continuous or discrete) outcome variable.</li>
<li>Regression models use a mean squared error (MSE) loss and classification models use a cross-entropy loss.</li>
</ul></li>
</ol></li>
<li>Results</li>
</ol>
<ul>
<li>This section presents the results of the experiments.
<ol type="1">
<li>Downstream Task Performance (RQ 1)
<ul>
<li>SatCLIP embeddings (in either L = 10 or L = 40 configuration) achieve the best prediction scores by a large margin on seven of the nine tasks.</li>
<li>The exceptions are the Cali. Housing dataset, which is limited to California, and the Median Income dataset, which only contains data from the continental United States (US).</li>
<li>SatCLIP performs well on all continents for both tasks.</li>
<li>Prior location encoders (CSP and GPS2Vec) trained on spatially biased training data tend to perform better in Europe and North America than in the underrepresented continents of Africa, Asia, and South America.</li>
</ul></li>
<li>Zero/Few-Shot Geographic Adaptation (RQ 2)
<ul>
<li>SatCLIP models are often (but not always) better than the comparison approaches across both held-out continents.</li>
<li>Overall, SatCLIP embeddings perform systematically better in the few-shot geographic adaptation setting for Ecoregions and Countries tasks.</li>
</ul></li>
<li>Analysis of Location Embeddings (RQ 3)
<ul>
<li>They investigate qualitatively to what degree the SatCLIP embeddings have learned an implicit representation of different ground conditions in the location encoder weights.</li>
<li>SatCLIP location embeddings show high similarity between the Congo Basin location and other areas close to the Equator, particularly the Amazon and Indonesia.</li>
<li>In comparison, embeddings of the North American location are similar to areas in Europe or northern China that are similarly population-dense and industrialized.</li>
</ul></li>
<li>Effect of Location and Image Encoder Design on Performance
<ul>
<li>The choice of image encoder for pretraining (ViT-16, ResNet-18, ResNet-50) appears to only marginally affect results.</li>
<li>In contrast, different location encoder resolutions with scale parameters L = 10 and L = 40 have a greater effect.</li>
</ul></li>
</ol></li>
</ul>
<ol start="5" type="1">
<li>Discussion
<ul>
<li>SatCLIP models can provide useful information for a wide range of downstream tasks, with the concrete benefit varying across tasks.</li>
<li>SatCLIP embeddings perform well across continents and are less prone to geographic bias in comparison to other methods like GPS2Vec or CSP.</li>
<li>The transfer of spatial patterns in Sentinel-2 imagery into the SatCLIP location encoder enables generalization across geographic areas.</li>
<li>At test time, SatCLIP location encoders can be applied directly to any point on the globe, without needing to download additional imagery.</li>
</ul></li>
<li>Conclusion
<ul>
<li>SatCLIP is a method to learn an implicit neural representation of visual patterns on the globe by matching satellite images and their respective coordinates using a contrastive location-image pretraining objective.</li>
<li>Experiments show that SatCLIP is effective for global prediction tasks spanning social and environmental domains, for both interpolation and out-of-sample geographic prediction.</li>
<li>Two key factors contribute to the performance:</li>
<li>Their S2-100K pretraining dataset is uniformly distributed across the globe.</li>
<li>They use the recently proposed Siren(SH) location encoder which has proven well-suited for the global-scale representation of data on the spherical Earth.</li>
</ul></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/mdominguezd\.github\.io\/cool_lit\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>